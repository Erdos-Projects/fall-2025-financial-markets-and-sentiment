{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdde5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atproto import Client\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Iterable, Optional\n",
    "import os, csv, time, requests\n",
    "\n",
    "# ---------- CONFIG / LOGIN ----------\n",
    "client = Client()\n",
    "client.login(\"YOUR USERNAME.bsky.social\", \"YOUR PASSWORD\")  # for production, consider env vars\n",
    "\n",
    "# Where to save results (macOS)\n",
    "SAVE_DIR = \"/Users/nihan/Desktop/bluesky\"\n",
    "\n",
    "# HTTP fallback base (public AppView)\n",
    "APPVIEW_BASE = \"https://public.api.bsky.app\"\n",
    "\n",
    "# ---------- COMPANIES & ALIASES ----------\n",
    "fortune_20 ={\n",
    "    1: \"Walmart\",\n",
    "    2: \"Amazon\",\n",
    "    3: \"Apple\",\n",
    "    4: \"CVS\",\n",
    "    5: \"Tesla\",\n",
    "    6: \"Google\",\n",
    "    7: \"Meta\",\n",
    "    8: \"JPMorgan\",\n",
    "    9: \"Costco\",\n",
    "    10: \"Kroger\",\n",
    "    11: \"Walgreens\",\n",
    "    12: \"Target\",\n",
    "    13: \"UPS\",\n",
    "    14: \"Centene\",\n",
    "    15: \"Cigna\",\n",
    "    16: \"Microsoft\",\n",
    "    17: \"Verizon\",\n",
    "    18: \"IBM\",\n",
    "    19: \"UnitedHealth\",\n",
    "    20: \"ExxonMobil\",\n",
    "    21: \"McKesson\",\n",
    "    22: \"Chevron\",\n",
    "    23: \"Cardinal Health\",\n",
    "    24: \"Home Depot\",\n",
    "    25: \"Walgreens Boots Alliance\",\n",
    "    26: \"Marathon Petroleum\",\n",
    "    27: \"Cencora\",\n",
    "    28: \"Ford\",\n",
    "    29: \"Citigroup\",\n",
    "    30: \"Dell Technologies\",\n",
    "    31: \"General Motors\",\n",
    "    32: \"Nvidia\",\n",
    "    33: \"Pfizer\",\n",
    "    34: \"Procter & Gamble\",\n",
    "    35: \"Comcast\",\n",
    "    36: \"Johnson & Johnson\",\n",
    "    37: \"Wells Fargo\",\n",
    "    38: \"Bank of America\",\n",
    "    39: \"AbbVie\",\n",
    "    40: \"Starbucks\",\n",
    "    41: \"Cisco\",\n",
    "    42: \"AT&T\",\n",
    "    43: \"PepsiCo\",\n",
    "    44: \"Intel\",\n",
    "    45: \"The Walt Disney Company\",\n",
    "    46: \"Boeing\",\n",
    "    47: \"Goldman Sachs\",\n",
    "    48: \"Morgan Stanley\",\n",
    "    49: \"Honeywell\",\n",
    "    50: \"Salesforce\"\n",
    "}\n",
    "\n",
    "COMPANIES = list(fortune_20.values())\n",
    "\n",
    "# Add this dict (aliases + ticker forms). Multi-word terms are auto-quoted by your _normalize_term.\n",
    "COMPANY_ALIASES = {\n",
    "    \"Walmart\": [\n",
    "        \"Walmart\", \"#Walmart\", \"WMT\", \"$WMT\", \"Doug McMillon\", \"Walmrt\", \"Walmrt#\", \"WMLT\"\n",
    "    ],\n",
    "    \"Amazon\": [\n",
    "        \"Amazon\", \"AMZN\", \"$AMZN\", \"#AMZN\", \"Amazon.com\", \"Andy Jassy\", \"Amzon\", \"Aamzon\", \"AMZON\", \"Jeff Bezos\"\n",
    "    ],\n",
    "    \"Apple\": [\n",
    "        \"Apple\", \"AAPL\", \"$AAPL\", \"#AAPL\", \"Apple Inc\", \"Tim Cook\", \"Aple\", \"Appl\", \"AAPLE\"\n",
    "    ],\n",
    "    \"CVS\": [\n",
    "        \"CVS\", \"CVS Health\", \"CVS Pharmacy\", \"CVS Health Corp\", \"CVS Health Corporation\", \"$CVS\", \"#CVS\", \"CVS\", \"David Joyner\", \"CVSS\", \"CV Health\"\n",
    "    ],\n",
    "    \"Tesla\": [\n",
    "        \"Tesla\", \"TSLA\", \"$TSLA\", \"#TSLA\", \"Tesla Motors\", \"Elon Musk\", \"Tesl\", \"Tsla\", \"TSLAA\"\n",
    "    ],\n",
    "    \"Google\": [\n",
    "        \"Google\", \"Alphabet Inc\", \"GOOGL\", \"$GOOGL\", \"#GOOGL\", \"Sundar Pichai\", \"Gooogle\", \"Gogle\", \"GOGL\"\n",
    "    ],\n",
    "    \"Meta\": [\n",
    "        \"Meta\", \"Meta Platforms\", \"META\", \"$META\", \"#META\", \"Facebook\", \"Mark Zuckerberg\", \"Metta\", \"Mett\", \"Metaa\"\n",
    "    ],\n",
    "    \"JPMorgan\": [\n",
    "        \"JPMorgan\", \"JP Morgan\", \"JPMorgan Chase\", \"JPMorgan Chase & Co.\", \"JPM\", \"$JPM\", \"#JPM\", \"Chase\", \"#Chase\", \"Jamie Dimon\", \"JPMO\", \"JP Morgan Chase\"\n",
    "    ],\n",
    "    \"Costco\": [\n",
    "        \"Costco\", \"Costco Wholesale\", \"COST\", \"$COST\", \"Ron Vachris\", \"Costcoo\", \"Costkco\"\n",
    "    ],\n",
    "    \"Kroger\": [\n",
    "        \"Kroger\", \"The Kroger Co.\", \"KR\", \"$KR\", \"Rodney McMullen\", \"Kroger Co\", \"Krogar\"\n",
    "    ],\n",
    "    \"Walgreens\": [\n",
    "        \"Walgreens\", \"WBA\", \"$WBA\", \"Tim Wentworth\", \"Walgreen\", \"Walgrens\"\n",
    "    ],\n",
    "    \"Target\": [\n",
    "        \"Target\", \"TGT\", \"$TGT\", \"Brian Cornell\", \"Targert\", \"Traget\"\n",
    "    ],\n",
    "    \"UPS\": [\n",
    "        \"UPS\", \"United Parcel Service\", \"$UPS\", \"#UPS\", \"Carol Tomé\", \"Carol Tome\", \"UPZ\", \"Upes\"\n",
    "    ],\n",
    "    \"Centene\": [\n",
    "        \"Centene\", \"Centene Corp\", \"Centene Corporation\", \"CNC\", \"$CNC\", \"Sarah London\", \"Centain\", \"Centnee\"\n",
    "    ],\n",
    "    \"Cigna\": [\n",
    "        \"Cigna\", \"The Cigna Group\", \"CI\", \"$CI\", \"David Cordani\", \"Cigna Group\", \"Cigna Health\"\n",
    "    ],\n",
    "    \"Microsoft\": [\n",
    "        \"Microsoft\", \"MSFT\", \"$MSFT\", \"#MSFT\", \"Satya Nadella\", \"Microsof\", \"Microsft\", \"Bill Gates\"\n",
    "    ],\n",
    "    \"Verizon\": [\n",
    "        \"Verizon\", \"Verizon Communications\", \"VZ\", \"$VZ\", \"Hans Vestberg\", \"Verizon Comm\", \"Verisone\"\n",
    "    ],\n",
    "    \"IBM\": [\n",
    "        \"IBM\", \"International Business Machines\", \"$IBM\", \"#IBM\", \"Arvind Krishna\", \"IBMM\", \"IBM Corp\"\n",
    "    ],\n",
    "    \"UnitedHealth\": [\n",
    "        \"UnitedHealth\", \"UnitedHealth Group\", \"United Healthcare\", \"UnitedHealthcare\", \"UNH\", \"$UNH\", \"#UNH\", \"Stephen Hemsley\", \"United Health\"\n",
    "    ],\n",
    "    \"ExxonMobil\": [\n",
    "        \"ExxonMobil\", \"Exxon Mobil Corporation\", \"XOM\", \"$XOM\", \"Darren Woods\", \"Exxon Mobile\", \"ExonnMobil\"\n",
    "    ],\n",
    "    \"McKesson\": [\n",
    "        \"McKesson\", \"McKesson Corp\", \"McKesson Corporation\", \"MCK\", \"$MCK\", \"Brian Tyler\", \"Mckesson Corporation\", \"McKessonCo\"\n",
    "    ],\n",
    "    \"Chevron\": [\n",
    "        \"Chevron\", \"CVX\", \"$CVX\", \"Michael Wirth\", \"Chevorn\", \"Chevero\"\n",
    "    ],\n",
    "    \"Cardinal Health\": [\n",
    "        \"Cardinal Health\", \"CAH\", \"$CAH\", \"Jason Hollar\", \"Cardnel Health\"\n",
    "    ],\n",
    "    \"Home Depot\": [\n",
    "        \"Home Depot\", \"The Home Depot\", \"HD\", \"$HD\", \"Ted Decker\", \"HomeDeopt\", \"Hom Depot\"\n",
    "    ],\n",
    "    \"Walgreens Boots Alliance\": [\n",
    "        \"Walgreens Boots Alliance\", \"WBA\", \"$WBA\", \"Tim Wentworth\", \"Walgreens Boots Alianace\"\n",
    "    ],\n",
    "    \"Marathon Petroleum\": [\n",
    "        \"Marathon Petroleum\", \"MPC\", \"$MPC\", \"Michael Hennigan\", \"Marathon Petroelum\"\n",
    "    ],\n",
    "    \"Cencora\": [\n",
    "        \"Cencora\", \"COR\", \"$COR\", \"Robert P. Mauch\", \"Cencora Inc\"\n",
    "    ],\n",
    "    \"Ford\": [\n",
    "        \"Ford\", \"Ford Motor Company\", \"F\", \"$F\", \"Jim Farley\", \"Frod\", \"Ford Co\"\n",
    "    ],\n",
    "    \"Citigroup\": [\n",
    "        \"Citigroup\", \"C\", \"$C\", \"Jane Fraser\", \"Citigroup Inc\"\n",
    "    ],\n",
    "    \"Dell Technologies\": [\n",
    "        \"Dell Technologies\", \"DELL\", \"$DELL\", \"Michael Dell\", \"Del Technologies\"\n",
    "    ],\n",
    "    \"General Motors\": [\n",
    "        \"General Motors\", \"GM\", \"$GM\", \"Mary Barra\", \"General Moters\"\n",
    "    ],\n",
    "    \"Nvidia\": [\n",
    "        \"Nvidia\", \"NVDA\", \"$NVDA\", \"Jensen Huang\", \"Nvidiaa\"\n",
    "    ],\n",
    "    \"Pfizer\": [\n",
    "        \"Pfizer\", \"PFE\", \"$PFE\", \"Albert Bourla\", \"Pfizzer\"\n",
    "    ],\n",
    "    \"Procter & Gamble\": [\n",
    "        \"Procter & Gamble\", \"P&G\", \"PG\", \"$PG\", \"Jon Moeller\", \"Procter and Gamble\", \"Procter & Gample\"\n",
    "    ],\n",
    "    \"Comcast\": [\n",
    "        \"Comcast\", \"CMCSA\", \"$CMCSA\", \"Brian Roberts\", \"Commcast\"\n",
    "    ],\n",
    "    \"Johnson & Johnson\": [\n",
    "        \"Johnson & Johnson\", \"J&J\", \"JNJ\", \"$JNJ\", \"Joaquin Duato\", \"Johnsons & Johnson\"\n",
    "    ],\n",
    "    \"Wells Fargo\": [\n",
    "        \"Wells Fargo\", \"WFC\", \"$WFC\", \"Charles Scharf\", \"Wells Fago\"\n",
    "    ],\n",
    "    \"Bank of America\": [\n",
    "        \"Bank of America\", \"BAC\", \"$BAC\", \"Brian Moynihan\", \"Bank of Amercia\"\n",
    "    ],\n",
    "    \"AbbVie\": [\n",
    "        \"AbbVie\", \"ABBV\", \"$ABBV\", \"Richard Gonzalez\", \"Abbvie\"\n",
    "    ],\n",
    "    \"Starbucks\": [\n",
    "        \"Starbucks\", \"SBUX\", \"$SBUX\", \"Kevin Johnson\", \"Starbuks\", \"Starbucs\", \"Starbuck\", \"Starbux\"\n",
    "    ],\n",
    "    \"Cisco\": [\n",
    "        \"Cisco\", \"Cisco Systems\", \"CSCO\", \"$CSCO\", \"Chuck Robbins\", \"Ciscco\"\n",
    "    ],\n",
    "    \"AT&T\": [\n",
    "        \"AT&T\", \"T\", \"$T\", \"John Stankey\", \"AT and T\"\n",
    "    ],\n",
    "    \"PepsiCo\": [\n",
    "        \"PepsiCo\", \"PEP\", \"$PEP\", \"Ramon Laguarta\", \"Pepsi Co\"\n",
    "    ],\n",
    "    \"Intel\": [\n",
    "        \"Intel\", \"INTC\", \"$INTC\", \"Pat Gelsinger\", \"Intell\"\n",
    "    ],\n",
    "    \"The Walt Disney Company\": [\n",
    "        \"Disney\", \"The Walt Disney Company\", \"DIS\", \"$DIS\", \"Bob Iger\", \"Diseny\"\n",
    "    ],\n",
    "    \"Boeing\": [\n",
    "        \"Boeing\", \"BA\", \"$BA\", \"David Calhoun\", \"Boieng\"\n",
    "    ],\n",
    "    \"Goldman Sachs\": [\n",
    "        \"Goldman Sachs\", \"GS\", \"$GS\", \"David Solomon\", \"Goldmann Sachs\"\n",
    "    ],\n",
    "    \"Morgan Stanley\": [\n",
    "        \"Morgan Stanley\", \"MS\", \"$MS\", \"Ted Pick\", \"Morgan Stanly\"\n",
    "    ],\n",
    "    \"Honeywell\": [\n",
    "        \"Honeywell\", \"Honeywell International\", \"HON\", \"$HON\", \"Vimal Kapur\", \"Honeywel\"\n",
    "    ],\n",
    "    \"Salesforce\": [\n",
    "        \"Salesforce\", \"CRM\", \"$CRM\", \"Marc Benioff\", \"SalesForce\", \"SaleForce\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "TICKER_ALIASES = {\n",
    "    \"Walmart\": [\"Walmart\", \"#Walmart\", \"WMT\", \"$WMT\", \"Walmrt\", \"Walmrt#\", \"WMLT\"],\n",
    "    \"Amazon\": [\"Amazon\", \"AMZN\", \"$AMZN\", \"#AMZN\", \"Amazon.com\", \"Amzon\", \"Aamzon\", \"AMZON\"],\n",
    "    \"Apple\": [\"Apple\", \"AAPL\", \"$AAPL\", \"#AAPL\", \"Apple Inc\", \"Aple\", \"Appl\", \"AAPLE\"],\n",
    "    \"CVS\": [\"CVS\", \"CVS Health\", \"CVS Pharmacy\", \"CVS Health Corp\", \"CVS Health Corporation\", \"$CVS\", \"#CVS\", \"CVSS\", \"CV Health\"],\n",
    "    \"Tesla\": [\"Tesla\", \"TSLA\", \"$TSLA\", \"#TSLA\", \"Tesla Motors\", \"Tesl\", \"Tsla\", \"TSLAA\"],\n",
    "    \"Google\": [\"Google\", \"Alphabet Inc\", \"GOOGL\", \"$GOOGL\", \"#GOOGL\", \"Gooogle\", \"Gogle\", \"GOGL\"],\n",
    "    \"Meta\": [\"Meta\", \"Meta Platforms\", \"META\", \"$META\", \"#META\", \"Facebook\", \"Metta\", \"Mett\", \"Metaa\"],\n",
    "    \"JPMorgan\": [\"JPMorgan\", \"JP Morgan\", \"JPMorgan Chase\", \"JPM\", \"$JPM\", \"#JPM\", \"JPMO\", \"JP Morgan Chase\"],\n",
    "    \"Costco\": [\"Costco\", \"Costco Wholesale\", \"COST\", \"$COST\", \"Costcoo\", \"Costkco\"],\n",
    "    \"Kroger\": [\"Kroger\", \"The Kroger Co.\", \"KR\", \"$KR\", \"Kroger Co\", \"Krogar\"],\n",
    "    \"Walgreens\": [\"Walgreens\", \"WBA\", \"$WBA\", \"Walgreen\", \"Walgrens\"],\n",
    "    \"Target\": [\"Target\", \"TGT\", \"$TGT\", \"Targert\", \"Traget\"],\n",
    "    \"UPS\": [\"UPS\", \"United Parcel Service\", \"$UPS\", \"#UPS\", \"UPZ\", \"Upes\"],\n",
    "    \"Centene\": [\"Centene\", \"Centene Corp\", \"Centene Corporation\", \"CNC\", \"$CNC\", \"Centain\", \"Centnee\"],\n",
    "    \"Cigna\": [\"Cigna\", \"The Cigna Group\", \"CI\", \"$CI\", \"Cigna Group\", \"Cigna Health\"],\n",
    "    \"Microsoft\": [\"Microsoft\", \"MSFT\", \"$MSFT\", \"#MSFT\", \"Microsof\", \"Microsft\"],\n",
    "    \"Verizon\": [\"Verizon\", \"Verizon Communications\", \"VZ\", \"$VZ\", \"Verizon Comm\", \"Verisone\"],\n",
    "    \"IBM\": [\"IBM\", \"International Business Machines\", \"$IBM\", \"#IBM\", \"IBMM\", \"IBM Corp\"],\n",
    "    \"UnitedHealth\": [\"UnitedHealth\", \"UnitedHealth Group\", \"United Healthcare\", \"UNH\", \"$UNH\", \"#UNH\", \"United Health\"],\n",
    "    \"ExxonMobil\": [\"ExxonMobil\", \"Exxon Mobil Corporation\", \"XOM\", \"$XOM\", \"Exxon Mobile\", \"ExonnMobil\"],\n",
    "    \"McKesson\": [\"McKesson\", \"McKesson Corp\", \"McKesson Corporation\", \"MCK\", \"$MCK\", \"Mckesson Corporation\", \"McKessonCo\"],\n",
    "    \"Chevron\": [\"Chevron\", \"CVX\", \"$CVX\", \"Chevorn\", \"Chevero\"],\n",
    "    \"Cardinal Health\": [\"Cardinal Health\", \"CAH\", \"$CAH\", \"Cardnel Health\"],\n",
    "    \"Home Depot\": [\"Home Depot\", \"The Home Depot\", \"HD\", \"$HD\", \"HomeDeopt\", \"Hom Depot\"],\n",
    "    \"Walgreens Boots Alliance\": [\"Walgreens Boots Alliance\", \"WBA\", \"$WBA\", \"Walgreens Boots Alianace\"],\n",
    "    \"Marathon Petroleum\": [\"Marathon Petroleum\", \"MPC\", \"$MPC\", \"Marathon Petroelum\"],\n",
    "    \"Cencora\": [\"Cencora\", \"COR\", \"$COR\", \"Cencora Inc\", \"Robert P. Mauch\"],\n",
    "    \"Ford\": [\"Ford\", \"Ford Motor Company\", \"F\", \"$F\", \"Frod\", \"Ford Co\"],\n",
    "    \"Citigroup\": [\"Citigroup\", \"C\", \"$C\", \"Citigroup Inc\", \"Jane Fraser\"],\n",
    "    \"Dell Technologies\": [\"Dell Technologies\", \"DELL\", \"$DELL\", \"Del Technologies\"],\n",
    "    \"General Motors\": [\"General Motors\", \"GM\", \"$GM\", \"General Moters\"],\n",
    "    \"Nvidia\": [\"Nvidia\", \"NVDA\", \"$NVDA\", \"Nvidiaa\"],\n",
    "    \"Pfizer\": [\"Pfizer\", \"PFE\", \"$PFE\", \"Pfizzer\"],\n",
    "    \"Procter & Gamble\": [\"Procter & Gamble\", \"P&G\", \"PG\", \"$PG\", \"Procter and Gamble\", \"Procter & Gample\"],\n",
    "    \"Comcast\": [\"Comcast\", \"CMCSA\", \"$CMCSA\", \"Commcast\"],\n",
    "    \"Johnson & Johnson\": [\"Johnson & Johnson\", \"J&J\", \"JNJ\", \"$JNJ\", \"Johnsons & Johnson\"],\n",
    "    \"Wells Fargo\": [\"Wells Fargo\", \"WFC\", \"$WFC\", \"Wells Fago\"],\n",
    "    \"Bank of America\": [\"Bank of America\", \"BAC\", \"$BAC\", \"Bank of Amercia\"],\n",
    "    \"AbbVie\": [\"AbbVie\", \"ABBV\", \"$ABBV\", \"Abbvie\"],\n",
    "    \"Starbucks\": [\"Starbucks\", \"SBUX\", \"$SBUX\", \"Starbuks\", \"Starbucs\", \"Starbuck\", \"Starbux\"],\n",
    "    \"Cisco\": [\"Cisco\", \"Cisco Systems\", \"CSCO\", \"$CSCO\", \"Ciscco\"],\n",
    "    \"AT&T\": [\"AT&T\", \"T\", \"$T\", \"AT and T\"],\n",
    "    \"PepsiCo\": [\"PepsiCo\", \"PEP\", \"$PEP\", \"Pepsi Co\"],\n",
    "    \"Intel\": [\"Intel\", \"INTC\", \"$INTC\", \"Intell\"],\n",
    "    \"The Walt Disney Company\": [\"Disney\", \"The Walt Disney Company\", \"DIS\", \"$DIS\", \"Diseny\"],\n",
    "    \"Boeing\": [\"Boeing\", \"BA\", \"$BA\", \"Boieng\"],\n",
    "    \"Goldman Sachs\": [\"Goldman Sachs\", \"GS\", \"$GS\", \"Goldmann Sachs\"],\n",
    "    \"Morgan Stanley\": [\"Morgan Stanley\", \"MS\", \"$MS\", \"Morgan Stanly\"],\n",
    "    \"Honeywell\": [\"Honeywell\", \"Honeywell International\", \"HON\", \"$HON\", \"Honeywel\"],\n",
    "    \"Salesforce\": [\"Salesforce\", \"CRM\", \"$CRM\", \"SalesForce\", \"SaleForce\"]\n",
    "}\n",
    "\n",
    "CEO_LIST = {\n",
    "    \"Walmart\": [\"Doug McMillon\"],\n",
    "    \"Amazon\": [\"Andy Jassy\", \"Jeff Bezos\"],\n",
    "    \"Apple\": [\"Tim Cook\"],\n",
    "    \"CVS\": [\"David Joyner\"],\n",
    "    \"Tesla\": [\"Elon Musk\"],\n",
    "    \"Google\": [\"Sundar Pichai\"],\n",
    "    \"Meta\": [\"Mark Zuckerberg\"],\n",
    "    \"JPMorgan\": [\"Jamie Dimon\"],\n",
    "    \"Costco\": [\"Ron Vachris\"],\n",
    "    \"Kroger\": [\"Rodney McMullen\"],\n",
    "    \"Walgreens\": [\"Tim Wentworth\"],\n",
    "    \"Target\": [\"Brian Cornell\"],\n",
    "    \"UPS\": [\"Carol Tomé\", \"Carol Tome\"],\n",
    "    \"Centene\": [\"Sarah London\"],\n",
    "    \"Cigna\": [\"David Cordani\"],\n",
    "    \"Microsoft\": [\"Satya Nadella\", \"Bill Gates\"],\n",
    "    \"Verizon\": [\"Hans Vestberg\"],\n",
    "    \"IBM\": [\"Arvind Krishna\"],\n",
    "    \"UnitedHealth\": [\"Stephen Hemsley\"],\n",
    "    \"ExxonMobil\": [\"Darren Woods\"],\n",
    "    \"McKesson\": [\"Brian Tyler\"],\n",
    "    \"Chevron\": [\"Michael Wirth\"],\n",
    "    \"Cardinal Health\": [\"Jason Hollar\"],\n",
    "    \"Home Depot\": [\"Ted Decker\"],\n",
    "    \"Walgreens Boots Alliance\": [\"Tim Wentworth\"],\n",
    "    \"Marathon Petroleum\": [\"Michael Hennigan\"],\n",
    "    \"Cencora\": [\"Robert P. Mauch\"],\n",
    "    \"Ford\": [\"Jim Farley\"],\n",
    "    \"Citigroup\": [\"Jane Fraser\"],\n",
    "    \"Dell Technologies\": [\"Michael Dell\"],\n",
    "    \"General Motors\": [\"Mary Barra\"],\n",
    "    \"Nvidia\": [\"Jensen Huang\"],\n",
    "    \"Pfizer\": [\"Albert Bourla\"],\n",
    "    \"Procter & Gamble\": [\"Jon Moeller\"],\n",
    "    \"Comcast\": [\"Brian Roberts\"],\n",
    "    \"Johnson & Johnson\": [\"Joaquin Duato\"],\n",
    "    \"Wells Fargo\": [\"Charles Scharf\"],\n",
    "    \"Bank of America\": [\"Brian Moynihan\"],\n",
    "    \"AbbVie\": [\"Richard Gonzalez\"],\n",
    "    \"Starbucks\": [\"Kevin Johnson\"],\n",
    "    \"Cisco\": [\"Chuck Robbins\"],\n",
    "    \"AT&T\": [\"John Stankey\"],\n",
    "    \"PepsiCo\": [\"Ramon Laguarta\"],\n",
    "    \"Intel\": [\"Pat Gelsinger\"],\n",
    "    \"The Walt Disney Company\": [\"Bob Iger\"],\n",
    "    \"Boeing\": [\"David Calhoun\"],\n",
    "    \"Goldman Sachs\": [\"David Solomon\"],\n",
    "    \"Morgan Stanley\": [\"Ted Pick\"],\n",
    "    \"Honeywell\": [\"Vimal Kapur\"],\n",
    "    \"Salesforce\": [\"Marc Benioff\"]\n",
    "}\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _parse_iso_created_at(maybe_iso: str) -> Optional[datetime]:\n",
    "    if not maybe_iso:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.fromisoformat(maybe_iso.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _post_created_at(post) -> Optional[datetime]:\n",
    "    try:\n",
    "        rec = getattr(post, \"record\", None)\n",
    "        if rec:\n",
    "            for name in (\"created_at\", \"createdAt\"):\n",
    "                val = getattr(rec, name, None)\n",
    "                if val:\n",
    "                    dt = _parse_iso_created_at(val)\n",
    "                    if dt:\n",
    "                        return dt\n",
    "    except Exception:\n",
    "        pass\n",
    "    for name in (\"created_at\", \"createdAt\"):\n",
    "        val = getattr(post, name, None)\n",
    "        if val:\n",
    "            dt = _parse_iso_created_at(val)\n",
    "            if dt:\n",
    "                return dt\n",
    "    return None\n",
    "\n",
    "def _get_post_uri(obj) -> Optional[str]:\n",
    "    \"\"\"Return a stable unique id for dedupe (SDK: .uri, HTTP: ['uri']).\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(\"uri\")\n",
    "    return getattr(obj, \"uri\", None)\n",
    "\n",
    "def _date_range_last_n_days(days: int, today: Optional[date] = None) -> List[date]:\n",
    "    if today is None:\n",
    "        today = datetime.now(timezone.utc).date()\n",
    "    return [(today - timedelta(days=i)) for i in range(days, 0, -1)]\n",
    "\n",
    "# NEW: explicit date range helper (inclusive start, exclusive end)\n",
    "def _date_range_between(start_date: date, end_date: date) -> List[date]:\n",
    "    \"\"\"Return all UTC dates between start_date and end_date (inclusive of start, exclusive of end).\"\"\"\n",
    "    delta = (end_date - start_date).days\n",
    "    return [(start_date + timedelta(days=i)) for i in range(delta)]\n",
    "\n",
    "def _normalize_term(term: str) -> str:\n",
    "    \"\"\"Wrap multi-word terms in quotes if not already.\"\"\"\n",
    "    t = term.strip()\n",
    "    if \" \" in t and not (t.startswith('\"') and t.endswith('\"')):\n",
    "        t = f'\"{t}\"'\n",
    "    return t\n",
    "\n",
    "def _q_for_day(base_q: str, day_utc: date) -> str:\n",
    "    \"\"\"Inject UTC window into q so the API only returns that day.\"\"\"\n",
    "    day1 = day_utc.isoformat()\n",
    "    day2 = (day_utc + timedelta(days=1)).isoformat()\n",
    "    return f'{base_q} since:{day1} until:{day2}'\n",
    "\n",
    "# ---------- RESILIENT SEARCH ----------\n",
    "_RETRYABLE_HTTP = {429, 500, 502, 503, 504}\n",
    "\n",
    "def _sdk_search(q: str, limit: int, cursor: Optional[str]) -> object:\n",
    "    params = {\"q\": q, \"limit\": limit}\n",
    "    if cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "    return client.app.bsky.feed.search_posts(params)\n",
    "\n",
    "def _http_search(q: str, limit: int, cursor: Optional[str]) -> dict:\n",
    "    params = {\"q\": q, \"limit\": limit}\n",
    "    if cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "    r = requests.get(f\"{APPVIEW_BASE}/xrpc/app.bsky.feed.searchPosts\", params=params, timeout=30)\n",
    "    if r.status_code in _RETRYABLE_HTTP:\n",
    "        r.raise_for_status()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _search_resilient(q: str, limit: int, cursor: Optional[str],\n",
    "                      sdk_tries: int = 3, http_tries: int = 3,\n",
    "                      backoff_base: float = 1.5):\n",
    "    \"\"\"\n",
    "    Try SDK with exponential backoff; on failure, fallback to HTTP with backoff.\n",
    "    Returns (posts, next_cursor, used_http: bool).\n",
    "    Posts are either SDK objects or dicts.\n",
    "    \"\"\"\n",
    "    # SDK\n",
    "    attempt = 0\n",
    "    while attempt < sdk_tries:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            resp = _sdk_search(q, limit, cursor)\n",
    "            posts = getattr(resp, \"posts\", []) or []\n",
    "            next_cur = getattr(resp, \"cursor\", None)\n",
    "            return posts, next_cur, False\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if any(code in msg for code in (\"429\", \"500\", \"502\", \"503\", \"504\", \"UpstreamFailure\")):\n",
    "                time.sleep(min(backoff_base ** attempt, 30))\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    # HTTP\n",
    "    attempt = 0\n",
    "    while attempt < http_tries:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            resp = _http_search(q, limit, cursor)\n",
    "            posts = (resp or {}).get(\"posts\") or []\n",
    "            next_cur = (resp or {}).get(\"cursor\")\n",
    "            return posts, next_cur, True\n",
    "        except requests.HTTPError as e:\n",
    "            status = getattr(e.response, \"status_code\", None)\n",
    "            if status in _RETRYABLE_HTTP:\n",
    "                time.sleep(min(backoff_base ** attempt, 30))\n",
    "                continue\n",
    "            raise\n",
    "        except requests.RequestException:\n",
    "            time.sleep(min(backoff_base ** attempt, 30))\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\"Search failed after SDK and HTTP retries\")\n",
    "\n",
    "# ---------- CORE ----------\n",
    "def count_mentions_last_n_days_for_terms(\n",
    "    terms: List[str],\n",
    "    days: int = 2,\n",
    "    max_pages: int = 50,\n",
    "    per_request_limit: int = 100,\n",
    "    start_date: Optional[date] = None,   # NEW (optional)\n",
    "    end_date: Optional[date] = None      # NEW (optional, exclusive)\n",
    ") -> Counter:\n",
    "    \"\"\"\n",
    "    Count unique posts per day for a list of search terms (aliases).\n",
    "    - If start_date/end_date are given, uses that exact range (start inclusive, end exclusive).\n",
    "    - Otherwise, uses the last `days` ending today.\n",
    "    - Dedupe by post URI across terms and pages.\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "\n",
    "    # Build day list (NEW logic)\n",
    "    if start_date and end_date:\n",
    "        day_list = _date_range_between(start_date, end_date)\n",
    "    else:\n",
    "        today = datetime.now(timezone.utc).date()\n",
    "        day_list = _date_range_last_n_days(days, today)\n",
    "\n",
    "    # Normalize terms (quote multi-word)\n",
    "    terms = [_normalize_term(t) for t in terms]\n",
    "\n",
    "    for d in day_list:\n",
    "        start_dt = datetime.combine(d, datetime.min.time(), tzinfo=timezone.utc)\n",
    "        end_dt = datetime.combine(d + timedelta(days=1), datetime.min.time(), tzinfo=timezone.utc)\n",
    "\n",
    "        seen_uris: set = set()\n",
    "        daily_total = 0\n",
    "\n",
    "        for base in terms:\n",
    "            cursor = None\n",
    "            pages = 0\n",
    "            q_day = _q_for_day(base, d)\n",
    "\n",
    "            while True:\n",
    "                if pages >= max_pages:\n",
    "                    break\n",
    "                pages += 1\n",
    "\n",
    "                try:\n",
    "                    posts, cursor, used_http = _search_resilient(q_day, per_request_limit, cursor)\n",
    "                except Exception:\n",
    "                    # Give up this term/day, move to next term\n",
    "                    break\n",
    "\n",
    "                if not posts:\n",
    "                    break\n",
    "\n",
    "                first = posts[0]\n",
    "                is_dict = isinstance(first, dict)\n",
    "\n",
    "                if is_dict:\n",
    "                    # HTTP\n",
    "                    for post in posts:\n",
    "                        uri = post.get(\"uri\")\n",
    "                        if not uri or uri in seen_uris:\n",
    "                            continue\n",
    "                        rec = post.get(\"record\") or {}\n",
    "                        dt = _parse_iso_created_at(rec.get(\"createdAt\") or rec.get(\"created_at\"))\n",
    "                        if dt and (start_dt <= dt < end_dt):\n",
    "                            seen_uris.add(uri)\n",
    "                            daily_total += 1\n",
    "                else:\n",
    "                    # SDK\n",
    "                    for post in posts:\n",
    "                        uri = _get_post_uri(post)\n",
    "                        if not uri or uri in seen_uris:\n",
    "                            continue\n",
    "                        dt = _post_created_at(post)\n",
    "                        if dt and (start_dt <= dt < end_dt):\n",
    "                            seen_uris.add(uri)\n",
    "                            daily_total += 1\n",
    "\n",
    "                if not cursor:\n",
    "                    break\n",
    "\n",
    "        counts[d] = daily_total\n",
    "\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77195aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrix to: /Users/nihan/Desktop/bluesky/bluesky_counts_matrix_20251001-20251003.csv\n",
      "\n",
      "company, 2025-10-01, 2025-10-02\n",
      "Walmart, 814, 757\n",
      "Amazon, 2710, 1176\n",
      "Apple, 4561, 4306\n"
     ]
    }
   ],
   "source": [
    "# Code to count mentions of all company aliases in posts.\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Iterable, Optional\n",
    "import os, csv\n",
    "\n",
    "\n",
    "def counts_for_companies(\n",
    "    companies: Iterable[str],\n",
    "    days: int = 2,\n",
    "    start_date: Optional[date] = None,   # NEW\n",
    "    end_date: Optional[date] = None      # NEW\n",
    ") -> Dict[str, Counter]:\n",
    "    out: Dict[str, Counter] = {}\n",
    "    for name in companies:\n",
    "        terms = COMPANY_ALIASES.get(name, [name])\n",
    "        out[name] = count_mentions_last_n_days_for_terms(\n",
    "            terms, days=days, start_date=start_date, end_date=end_date\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def write_company_date_matrix_csv(\n",
    "    company_to_counts: Dict[str, Counter],\n",
    "    days: int,\n",
    "    save_dir: str,\n",
    "    filename_prefix: str = \"bluesky_counts_matrix\",\n",
    "    start_date: Optional[date] = None,   # NEW\n",
    "    end_date: Optional[date] = None      # NEW\n",
    ") -> str:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Build date header (NEW logic)\n",
    "    if start_date and end_date:\n",
    "        dates = _date_range_between(start_date, end_date)\n",
    "        filename = f\"{filename_prefix}_{start_date.strftime('%Y%m%d')}-{end_date.strftime('%Y%m%d')}.csv\"\n",
    "    else:\n",
    "        today = datetime.now(timezone.utc).date()\n",
    "        start_day = today - timedelta(days=days)\n",
    "        dates = _date_range_last_n_days(days, today)\n",
    "        filename = f\"{filename_prefix}_{start_day.strftime('%Y%m%d')}-{today.strftime('%Y%m%d')}.csv\"\n",
    "\n",
    "    header = [\"company\"] + [d.isoformat() for d in dates]\n",
    "    path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for company, cnt in company_to_counts.items():\n",
    "            row = [company] + [cnt.get(d, 0) for d in dates]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return path\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # EITHER use explicit dates (NEW):\n",
    "    START_DATE = date(2024, 8, 1)   # inclusive\n",
    "    END_DATE   = date(2025, 7, 31)   # exclusive\n",
    "    DAYS = 2  # kept for compatibility; ignored when start/end provided\n",
    "\n",
    "    company_counts = counts_for_companies(\n",
    "        COMPANIES, days=DAYS, start_date=START_DATE, end_date=END_DATE\n",
    "    )\n",
    "    csv_path = write_company_date_matrix_csv(\n",
    "        company_counts, DAYS, SAVE_DIR,\n",
    "        start_date=START_DATE, end_date=END_DATE\n",
    "    )\n",
    "\n",
    "    print(f\"Saved matrix to: {csv_path}\\n\")\n",
    "\n",
    "    # Build the same date list for printing\n",
    "    dates = _date_range_between(START_DATE, END_DATE)\n",
    "    print(\"company,\", \", \".join(d.isoformat() for d in dates))\n",
    "    for c in COMPANIES:\n",
    "        cnt = company_counts.get(c, Counter())\n",
    "        row = [str(cnt.get(d, 0)) for d in dates]\n",
    "        print(f\"{c}, \" + \", \".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f18dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrix to: /Users/nihan/Desktop/bluesky/bluesky_ticker_counts_matrix_20251001-20251003.csv\n",
      "\n",
      "company, 2025-10-01, 2025-10-02\n",
      "Walmart, 814, 757\n",
      "Amazon, 2630, 1043\n",
      "Apple, 4550, 4290\n"
     ]
    }
   ],
   "source": [
    "# Code to count mentions of containing ticker and/or stock name in posts. Not including posts with CEO name only.\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Iterable, Optional\n",
    "import os, csv\n",
    "\n",
    "# Reuse helpers via import in your project, or paste the same helpers here:\n",
    "# _date_range_last_n_days, _date_range_between, count_mentions_last_n_days_for_terms, TICKER_ALIASES, COMPANIES, SAVE_DIR\n",
    "\n",
    "def counts_for_tickers(\n",
    "    companies: Iterable[str],\n",
    "    days: int = 2,\n",
    "    start_date: Optional[date] = None,   # NEW\n",
    "    end_date: Optional[date] = None      # NEW\n",
    ") -> Dict[str, Counter]:\n",
    "    out: Dict[str, Counter] = {}\n",
    "    for name in companies:\n",
    "        terms = TICKER_ALIASES.get(name, [name])\n",
    "        out[name] = count_mentions_last_n_days_for_terms(\n",
    "            terms, days=days, start_date=start_date, end_date=end_date\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def write_company_date_matrix_csv(\n",
    "    tickers_to_counts: Dict[str, Counter],\n",
    "    days: int,\n",
    "    save_dir: str,\n",
    "    filename_prefix: str = \"bluesky_ticker_counts_matrix\",\n",
    "    start_date: Optional[date] = None,   # NEW\n",
    "    end_date: Optional[date] = None      # NEW\n",
    ") -> str:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if start_date and end_date:\n",
    "        dates = _date_range_between(start_date, end_date)\n",
    "        filename = f\"{filename_prefix}_{start_date.strftime('%Y%m%d')}-{end_date.strftime('%Y%m%d')}.csv\"\n",
    "    else:\n",
    "        today = datetime.now(timezone.utc).date()\n",
    "        start_day = today - timedelta(days=days)\n",
    "        dates = _date_range_last_n_days(days, today)\n",
    "        filename = f\"{filename_prefix}_{start_day.strftime('%Y%m%d')}-{today.strftime('%Y%m%d')}.csv\"\n",
    "\n",
    "    header = [\"company\"] + [d.isoformat() for d in dates]\n",
    "    path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for company, cnt in tickers_to_counts.items():\n",
    "            row = [company] + [cnt.get(d, 0) for d in dates]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return path\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    START_DATE = date(2024, 8, 1)   # inclusive\n",
    "    END_DATE   = date(2025, 7, 31)   # exclusive\n",
    "    DAYS = 2  # kept for compatibility\n",
    "\n",
    "    ticker_counts = counts_for_tickers(\n",
    "        COMPANIES, days=DAYS, start_date=START_DATE, end_date=END_DATE\n",
    "    )\n",
    "    csv_path = write_company_date_matrix_csv(\n",
    "        ticker_counts, DAYS, SAVE_DIR, start_date=START_DATE, end_date=END_DATE\n",
    "    )\n",
    "\n",
    "    print(f\"Saved matrix to: {csv_path}\\n\")\n",
    "\n",
    "    dates = _date_range_between(START_DATE, END_DATE)\n",
    "    print(\"company,\", \", \".join(d.isoformat() for d in dates))\n",
    "    for c in COMPANIES:\n",
    "        cnt = ticker_counts.get(c, Counter())\n",
    "        row = [str(cnt.get(d, 0)) for d in dates]\n",
    "        print(f\"{c}, \" + \", \".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba67f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrix to: /Users/nihan/Desktop/bluesky/bluesky_ceo_counts_matrix_20251001-20251003.csv\n",
      "\n",
      "company, 2025-10-01, 2025-10-02\n",
      "Walmart, 1, 1\n",
      "Amazon, 82, 136\n",
      "Apple, 15, 29\n"
     ]
    }
   ],
   "source": [
    "# Code to count mentions of CEO names only in posts (excluding posts with ticker or stock name ONLY).\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Iterable, Optional\n",
    "import os, csv\n",
    "\n",
    "# Reuse helpers via import in your project, or paste the same helpers here:\n",
    "# _date_range_last_n_days, _date_range_between, count_mentions_last_n_days_for_terms, CEO_LIST, COMPANIES, SAVE_DIR\n",
    "\n",
    "def counts_for_ceos(\n",
    "    companies: Iterable[str],\n",
    "    days: int = 2,\n",
    "    start_date: Optional[date] = None,   # NEW\n",
    "    end_date: Optional[date] = None      # NEW\n",
    ") -> Dict[str, Counter]:\n",
    "    out: Dict[str, Counter] = {}\n",
    "    for name in companies:\n",
    "        terms = CEO_LIST.get(name, [name])\n",
    "        out[name] = count_mentions_last_n_days_for_terms(\n",
    "            terms, days=days, start_date=start_date, end_date=end_date\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def write_company_date_matrix_csv(\n",
    "    ceo_to_counts: Dict[str, Counter],\n",
    "    days: int,\n",
    "    save_dir: str,\n",
    "    filename_prefix: str = \"bluesky_ceo_counts_matrix\",\n",
    "    start_date: Optional[date] = None,   # NEW\n",
    "    end_date: Optional[date] = None      # NEW\n",
    ") -> str:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if start_date and end_date:\n",
    "        dates = _date_range_between(start_date, end_date)\n",
    "        filename = f\"{filename_prefix}_{start_date.strftime('%Y%m%d')}-{end_date.strftime('%Y%m%d')}.csv\"\n",
    "    else:\n",
    "        today = datetime.now(timezone.utc).date()\n",
    "        start_day = today - timedelta(days=days)\n",
    "        dates = _date_range_last_n_days(days, today)\n",
    "        filename = f\"{filename_prefix}_{start_day.strftime('%Y%m%d')}-{today.strftime('%Y%m%d')}.csv\"\n",
    "\n",
    "    header = [\"company\"] + [d.isoformat() for d in dates]\n",
    "    path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for company, cnt in ceo_to_counts.items():\n",
    "            row = [company] + [cnt.get(d, 0) for d in dates]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return path\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    START_DATE = date(2024, 8, 1)   # inclusive\n",
    "    END_DATE   = date(2025, 7, 31)   # exclusive\n",
    "    DAYS = 2  # kept for compatibility\n",
    "\n",
    "    ceo_counts = counts_for_ceos(\n",
    "        COMPANIES, days=DAYS, start_date=START_DATE, end_date=END_DATE\n",
    "    )\n",
    "    csv_path = write_company_date_matrix_csv(\n",
    "        ceo_counts, DAYS, SAVE_DIR, start_date=START_DATE, end_date=END_DATE\n",
    "    )\n",
    "\n",
    "    print(f\"Saved matrix to: {csv_path}\\n\")\n",
    "\n",
    "    dates = _date_range_between(START_DATE, END_DATE)\n",
    "    print(\"company,\", \", \".join(d.isoformat() for d in dates))\n",
    "    for c in COMPANIES:\n",
    "        cnt = ceo_counts.get(c, Counter())\n",
    "        row = [str(cnt.get(d, 0)) for d in dates]\n",
    "        print(f\"{c}, \" + \", \".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f7dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee982e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
