{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840e0318",
   "metadata": {},
   "source": [
    "Step 1: Define Social Media Spikes\n",
    "Identify spike days/weeks/months where social media post volume exceeds the rolling historical mean by more than two standard deviations, indicating statistically significant bursts of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf969734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  all_keywords_mentions ticker\n",
      "0  2024-08-01                   1363   AAPL\n",
      "1  2024-08-02                   1409   AAPL\n",
      "2  2024-08-03                   1277   AAPL\n",
      "3  2024-08-04                   1221   AAPL\n",
      "4  2024-08-05                   1347   AAPL\n",
      "5  2024-08-06                   1322   AAPL\n",
      "6  2024-08-07                   1373   AAPL\n",
      "7  2024-08-08                   1289   AAPL\n",
      "8  2024-08-09                   1322   AAPL\n",
      "9  2024-08-10                   1147   AAPL\n",
      "10 2024-08-11                   1044   AAPL\n",
      "11 2024-08-12                   2176   AAPL\n",
      "12 2024-08-13                   1647   AAPL\n",
      "13 2024-08-14                   1579   AAPL\n",
      "14 2024-08-15                   1452   AAPL\n",
      "15 2024-08-16                   1467   AAPL\n",
      "16 2024-08-17                   1216   AAPL\n",
      "17 2024-08-18                   1285   AAPL\n",
      "18 2024-08-19                   1470   AAPL\n",
      "19 2024-08-20                   1421   AAPL\n",
      "20 2024-08-21                   1361   AAPL\n",
      "21 2024-08-22                   1321   AAPL\n",
      "22 2024-08-23                   1403   AAPL\n",
      "23 2024-08-24                   1337   AAPL\n",
      "24 2024-08-25                   1269   AAPL\n",
      "25 2024-08-26                   1538   AAPL\n",
      "26 2024-08-27                   1552   AAPL\n",
      "27 2024-08-28                   1454   AAPL\n",
      "28 2024-08-29                   1601   AAPL\n",
      "29 2024-08-30                   1647   AAPL\n",
      "30 2024-08-31                    223   AAPL\n",
      "31 2024-09-01                    503   AAPL\n",
      "32 2024-09-02                   2591   AAPL\n",
      "33 2024-09-03                   2842   AAPL\n",
      "34 2024-09-04                   2594   AAPL\n",
      "35 2024-09-05                   2575   AAPL\n",
      "36 2024-09-06                   2606   AAPL\n",
      "37 2024-09-07                   2219   AAPL\n",
      "38 2024-09-08                   2162   AAPL\n",
      "39 2024-09-09                   4854   AAPL\n",
      "40 2024-09-10                   4422   AAPL\n",
      "41 2024-09-11                   2611   AAPL\n",
      "42 2024-09-12                   2483   AAPL\n",
      "43 2024-09-13                   2664   AAPL\n",
      "44 2024-09-14                   2267   AAPL\n",
      "45 2024-09-15                   2058   AAPL\n",
      "46 2024-09-16                   2849   AAPL\n",
      "47 2024-09-17                   3006   AAPL\n",
      "48 2024-09-18                   2645   AAPL\n",
      "49 2024-09-19                   2552   AAPL\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and melt dataset as before\n",
    "df = pd.read_csv('bluesky_allkeywords.csv')\n",
    "\n",
    "df_long = pd.melt(\n",
    "    df,\n",
    "    id_vars=['company'],\n",
    "    var_name='date',\n",
    "    value_name='all_keywords_mentions'\n",
    ")\n",
    "df_long['date'] = pd.to_datetime(df_long['date'])\n",
    "\n",
    "# Mapping company to ticker explicitly\n",
    "sp500_tickers = [\"WMT\", \"AMZN\", \"AAPL\", \"CVS\", \"TSLA\", \"GOOGL\", \"META\", \"JPM\", \"COST\", \"KR\",\n",
    "\"WBA\", \"TGT\", \"UPS\", \"CNC\", \"CI\", \"MSFT\", \"VZ\", \"IBM\", \"UNH\", \"XOM\",\n",
    "\"MCK\", \"CVX\", \"CAH\", \"HD\", \"WBA\", \"MPC\", \"COR\", \"F\", \"C\", \"DELL\",\n",
    "\"GM\", \"NVDA\", \"PFE\", \"PG\", \"CMCSA\", \"JNJ\", \"WFC\", \"BAC\", \"ABBV\", \"SBUX\",\n",
    "\"CSCO\", \"T\", \"PEP\", \"INTC\", \"DIS\", \"BA\", \"GS\", \"MS\", \"HON\", \"CRM\"\n",
    "]\n",
    "company_names = df_long['company'].unique()\n",
    "company_to_ticker = dict(zip(company_names, sp500_tickers))\n",
    "df_long['ticker'] = df_long['company'].map(company_to_ticker)\n",
    "\n",
    "# Drop the original company column\n",
    "df_long = df_long.drop(columns=['company'])\n",
    "\n",
    "# Sort for rolling calculation\n",
    "df_long = df_long.sort_values(['ticker', 'date']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8cefea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date ticker  all_keywords_mentions\n",
      "18  2025-02-28   ABBV                    220\n",
      "20  2025-04-30   ABBV                    289\n",
      "21  2025-05-31   ABBV                    318\n",
      "23  2025-07-31   ABBV                    450\n",
      "45  2025-05-31     BA                  50827\n",
      "58  2025-06-30    BAC                   6468\n",
      "102 2025-02-28  CMCSA                   4425\n",
      "162 2025-02-28   CSCO                   2173\n",
      "258 2025-02-28     GS                   8106\n",
      "307 2025-03-31   INTC                  27792\n",
      "365 2025-01-31   META                 269928\n",
      "379 2025-03-31    MPC                   1254\n",
      "401 2025-01-31   MSFT                  50940\n",
      "413 2025-01-31   NVDA                  26116\n",
      "436 2024-12-31    PFE                   2488\n",
      "460 2024-12-31   SBUX                  17332\n",
      "498 2025-02-28   TSLA                 255432\n",
      "499 2025-03-31   TSLA                 290416\n",
      "508 2024-12-31    UNH                  52068\n",
      "534 2025-02-28     VZ                   6054\n",
      "545 2025-01-31    WBA                   8086\n",
      "556 2024-12-31    WFC                   1602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thisu\\AppData\\Local\\Temp\\ipykernel_17244\\793306762.py:20: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df_monthly = df_long.set_index('date').groupby('ticker')['all_keywords_mentions'].resample('M').sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spike flagging function\n",
    "def flag_spikes(df, window_size, count_col, flag_col):\n",
    "    rolling_mean = df.groupby('ticker')[count_col].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "    rolling_std = df.groupby('ticker')[count_col].transform(lambda x: x.rolling(window=window_size, min_periods=1).std().fillna(0))\n",
    "    df[flag_col] = df[count_col] > (rolling_mean + 1.5 * rolling_std)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Daily spikes\n",
    "df_long = flag_spikes(df_long, 7, 'all_keywords_mentions', 'daily_spike')\n",
    "daily_spikes = df_long[df_long['daily_spike']][['date', 'ticker', 'all_keywords_mentions']]\n",
    "\n",
    "# Weekly spikes\n",
    "df_weekly = df_long.set_index('date').groupby('ticker')['all_keywords_mentions'].resample('W').sum().reset_index()\n",
    "df_weekly = df_weekly.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "df_weekly = flag_spikes(df_weekly, 21, 'all_keywords_mentions', 'weekly_spike')\n",
    "weekly_spikes = df_weekly[df_weekly['weekly_spike']][['date', 'ticker', 'all_keywords_mentions']]\n",
    "\n",
    "# Monthly spikes\n",
    "df_monthly = df_long.set_index('date').groupby('ticker')['all_keywords_mentions'].resample('M').sum().reset_index()\n",
    "df_monthly = df_monthly.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "df_monthly = flag_spikes(df_monthly, 30, 'all_keywords_mentions', 'monthly_spike')\n",
    "monthly_spikes = df_monthly[df_monthly['monthly_spike']][['date', 'ticker', 'all_keywords_mentions']]\n",
    "\n",
    "print(monthly_spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccda1d4",
   "metadata": {},
   "source": [
    "Step 2: Estimate Post-Spike Returns and Volatility\n",
    "\n",
    "To estimate log returns, use the ??????\n",
    "To estimate weekly volatility, use the average weekly volatility for the previous 5 days.\n",
    "To estimate monthly volatility, use the average monthly volatility for the previous 21 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0010d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thisu\\AppData\\Local\\Temp\\ipykernel_17244\\2395438683.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  weekly_sigma_spike = pd.concat([weekly_sigma_spike, new_row], ignore_index=True)\n",
      "C:\\Users\\Thisu\\AppData\\Local\\Temp\\ipykernel_17244\\2395438683.py:60: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  monthly_sigma_spike = pd.concat([monthly_sigma_spike, new_row], ignore_index=True)\n",
      "C:\\Users\\Thisu\\AppData\\Local\\Temp\\ipykernel_17244\\2395438683.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  daily_sigma_spike = pd.concat([daily_sigma_spike, new_row], ignore_index=True)\n",
      "C:\\Users\\Thisu\\AppData\\Local\\Temp\\ipykernel_17244\\2395438683.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  daily_sigma_spike = pd.concat([daily_sigma_spike, new_row], ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "9468",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Thisu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_range\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[31mValueError\u001b[39m: 9468 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m date = row[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     69\u001b[39m ticker = row[\u001b[33m'\u001b[39m\u001b[33mcompany\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m historical_daily_volatility = [\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlog_return\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m4\u001b[39m,\u001b[32m5\u001b[39m,\u001b[32m6\u001b[39m,\u001b[32m7\u001b[39m,\u001b[32m8\u001b[39m,\u001b[32m9\u001b[39m,\u001b[32m10\u001b[39m]]\n\u001b[32m     71\u001b[39m expected_daily_volatility = pd.Series(historical_daily_volatility).mean()\n\u001b[32m     72\u001b[39m actual_volatility = get_value(stock_data, inx + \u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlog_return\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mget_value\u001b[39m\u001b[34m(df, index, column_name)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_value\u001b[39m(df, index, column_name):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     row = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m row[column_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Thisu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Thisu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1431\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[32m   1430\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Thisu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1381\u001b[39m, in \u001b[36m_LocIndexer._get_label\u001b[39m\u001b[34m(self, label, axis)\u001b[39m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Thisu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:4320\u001b[39m, in \u001b[36mNDFrame.xs\u001b[39m\u001b[34m(self, key, axis, level, drop_level)\u001b[39m\n\u001b[32m   4318\u001b[39m             new_index = index[loc]\n\u001b[32m   4319\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4320\u001b[39m     loc = \u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np.ndarray):\n\u001b[32m   4323\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m loc.dtype == np.bool_:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Thisu\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    413\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._range.index(new_key)\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 9468"
     ]
    }
   ],
   "source": [
    "###FIX CODE, LOAD IN DATA, THEN DO VOLATILITY ESTIMATION\n",
    "S\n",
    "\n",
    "\n",
    "##Code to get corresponding trading days before/after spikes\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import numpy as np\n",
    "\n",
    "#Now, to find trading day info for spikes\n",
    "#Make an estimation for \"expected\" weekly volatility and monthly volatility using rolling averages of historical data\n",
    "stock_data = pd.read_csv('bluesky_stock_merged.csv') \n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date']).dt.normalize()\n",
    "\n",
    "#Function to get row values from stock_data DataFrame\n",
    "def get_value(df, index, column_name):\n",
    "    row = df.loc[index]\n",
    "    return row[column_name]\n",
    "\n",
    "\n",
    "columns = ['Date', 'Ticker', 'Spike', 'AbnormalVolatility']\n",
    "weekly_sigma_spike = pd.DataFrame(columns=columns)\n",
    "\n",
    "#Weekly volatility estimation\n",
    "for inx in weekly_spikes.index:\n",
    "    row = weekly_spikes.loc[inx]\n",
    "    date = row['date']\n",
    "    ticker = row['company']\n",
    "    historical_weekly_volatility = [get_value(stock_data, inx - i, 'weekly_variance') for i in [1,2,3,4,5]]\n",
    "    expected_weekly_volatility = pd.Series(historical_weekly_volatility).mean()\n",
    "    actual_volatility = get_value(stock_data, inx + 5, 'weekly_variance')\n",
    "    abnormal_volatility = np.abs(actual_volatility - expected_weekly_volatility) if actual_volatility and expected_weekly_volatility else None\n",
    "    indicator = 1\n",
    "    new_row = pd.DataFrame({\n",
    "        'Date': [date],\n",
    "        'Ticker': [ticker],\n",
    "       'WeeklyVariance': [expected_weekly_volatility],\n",
    "        'Spike':[indicator],\n",
    "        'AbnormalVolatility': [abnormal_volatility]\n",
    "    })\n",
    "    weekly_sigma_spike = pd.concat([weekly_sigma_spike, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "#repeat for monthly volatility estimation\n",
    "columns = ['Date', 'Ticker', 'Spike', 'AbnormalVolatility']\n",
    "monthly_sigma_spike = pd.DataFrame(columns=columns)\n",
    "\n",
    "for inx in monthly_spikes.index:\n",
    "    row = monthly_spikes.loc[inx]\n",
    "    date = row['date']\n",
    "    ticker = row['company']\n",
    "    historical_monthly_volatility = [get_value(stock_data, inx - i, 'monthly_variance') for i in [1,2,3,4,5,6,7]]\n",
    "    expected_monthly_volatility = pd.Series(historical_monthly_volatility).mean()\n",
    "    actual_volatility = get_value(stock_data, inx + 21, 'monthly_variance')\n",
    "    abnormal_volatility = np.abs(actual_volatility - expected_monthly_volatility) if actual_volatility and expected_monthly_volatility else None\n",
    "    indicator = 1\n",
    "    new_row = pd.DataFrame({\n",
    "        'Date': [date],\n",
    "        'Ticker': [ticker],\n",
    "       'MonthlyVariance': [expected_monthly_volatility],\n",
    "        'Spike':[indicator],\n",
    "        'AbnormalVolatility': [abnormal_volatility]\n",
    "    })\n",
    "    monthly_sigma_spike = pd.concat([monthly_sigma_spike, new_row], ignore_index=True)\n",
    "\n",
    "#repeat for daily volatility estimation\n",
    "columns = ['Date', 'Ticker', 'Spike', 'AbnormalVolatility']\n",
    "daily_sigma_spike = pd.DataFrame(columns=columns)\n",
    "\n",
    "for inx in daily_spikes.index:\n",
    "    row = daily_spikes.loc[inx]\n",
    "    date = row['date']\n",
    "    ticker = row['company']\n",
    "    historical_daily_volatility = [get_value(stock_data, inx - i, 'log_return') for i in [1,2,3,4,5,6,7,8,9,10]]\n",
    "    expected_daily_volatility = pd.Series(historical_daily_volatility).mean()\n",
    "    actual_volatility = get_value(stock_data, inx + 1, 'log_return')\n",
    "    abnormal_volatility = np.abs(actual_volatility - expected_daily_volatility) if actual_volatility and expected_daily_volatility else None\n",
    "    indicator = 1\n",
    "    new_row = pd.DataFrame({\n",
    "        'Date': [date],\n",
    "        'Ticker': [ticker],\n",
    "       'DailyVariance': [expected_daily_volatility],\n",
    "        'Spike':[indicator],\n",
    "        'AbnormalVolatility': [abnormal_volatility]\n",
    "    })\n",
    "    daily_sigma_spike = pd.concat([daily_sigma_spike, new_row], ignore_index=True)\n",
    "\n",
    "print(daily_sigma_spike)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f2953",
   "metadata": {},
   "source": [
    "Step 3: Compare realized volatility/returns with estimated returns/volatility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62152d48",
   "metadata": {},
   "source": [
    "Step 4: Regress abnormal returns and abnormal volatility on social media spike indicators controlling for market index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
